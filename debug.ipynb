{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "# Jupyter notebook for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Copied from `train` function in train_simple.py:L78\n",
        "import yaml\n",
        "\n",
        "device = 'cpu'\n",
        "hyp = 'data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp, errors=\"ignore\") as f:\n",
        "    hyp = yaml.safe_load(f)  # load hyps dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=4 with nc=15\n",
            "Overriding model.yaml anchors with anchors=[[8.700976371765137, 18.705415725708008], [17.866653442382812, 14.511322975158691], [18.120155334472656, 34.65727996826172], [33.7895393371582, 20.528886795043945], [41.03347396850586, 31.010244369506836], [40.75947952270508, 67.85029602050781], [87.73966979980469, 33.539337158203125], [115.51000213623047, 81.31227111816406], [148.4618682861328, 222.72320556640625]]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "type list doesn't define __round__ method",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[109], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m check_dataset(data)\n\u001b[1;32m      8\u001b[0m nc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# number of classes\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manchors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# create\u001b[39;00m\n",
            "File \u001b[0;32m~/BAEK/AUE8088-PA2-main/AUE8088-PA2-main/models/yolo.py:237\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[0;34m(self, cfg, ch, nc, anchors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m anchors:\n\u001b[1;32m    236\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverriding model.yaml anchors with anchors=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchors\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# override yaml value\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;241m=\u001b[39m parse_model(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml), ch\u001b[38;5;241m=\u001b[39m[ch])  \u001b[38;5;66;03m# model, savelist\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m\"\u001b[39m])]  \u001b[38;5;66;03m# default names\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: type list doesn't define __round__ method"
          ]
        }
      ],
      "source": [
        "from models.yolo import Model\n",
        "from utils.general import check_dataset\n",
        "\n",
        "cfg = 'models/yolov5n_nuscenes.yaml'\n",
        "data = 'data/nuscenes.yaml'\n",
        "data_dict = check_dataset(data)\n",
        "\n",
        "nc = int(data_dict[\"nc\"])  # number of classes\n",
        "model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchors = model.model[-1].anchors\n",
        "\n",
        "# [TODO] Draw anchors\n",
        "anchors = model.model[-1].anchors\n",
        "\n",
        "# [TODO] Draw anchors\n",
        "# Plotting the anchors\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for i, (w, h) in enumerate(anchors):\n",
        "    rect = plt.Rectangle((0, 0), w, h, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    plt.text(w/2, h/2, f'Anchor {i+1}', color='blue', ha='center')\n",
        "\n",
        "plt.xlim(0, 5)\n",
        "plt.ylim(0, 7)\n",
        "plt.xlabel('Width')\n",
        "plt.ylabel('Height')\n",
        "plt.title('YOLO Anchors')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('/mnt/data/yolo_anchors_plot.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/datasets/nuscenes_det2d/train... 28130 images, 1425 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28130/28130 [00:01<00:00, 20217.09it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ubuntu/datasets/nuscenes_det2d/train.cache\n"
          ]
        }
      ],
      "source": [
        "from utils.dataloaders import create_dataloader\n",
        "from utils.general import check_img_size, colorstr\n",
        "\n",
        "imgsz = 416\n",
        "batch_size = 1\n",
        "single_cls = False\n",
        "seed = 0\n",
        "\n",
        "train_path = data_dict[\"train\"]\n",
        "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "imgsz = check_img_size(imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
        "\n",
        "train_loader, dataset = create_dataloader(\n",
        "    train_path,\n",
        "    imgsz,\n",
        "    batch_size,\n",
        "    gs,\n",
        "    single_cls,\n",
        "    hyp=hyp,\n",
        "    augment=True,\n",
        "    cache=None,\n",
        "    rect=False,\n",
        "    rank=-1,\n",
        "    workers=8,\n",
        "    image_weights=False,\n",
        "    quad=False,\n",
        "    prefix=colorstr(\"train: \"),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "for imgs, targets, paths, _ in train_loader:\n",
        "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ v7.0-320-g77b4eb3a Python-3.10.12 torch-2.0.1 CPU\n",
            "\n",
            "Fusing layers... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.torch_utils import select_device\n",
        "\n",
        "weights = 'yolov5n.pt'\n",
        "# data = 'data/nuscenes.yaml'\n",
        "data = 'data/coco128.yaml'\n",
        "half = False  # use FP16 half-precision inference\n",
        "dnn = False  # use OpenCV DNN for ONNX inference\n",
        "device = select_device('cpu')\n",
        "\n",
        "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(imgs)  # forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.general import non_max_suppression\n",
        "\n",
        "conf_thres = 0.25  # confidence threshold\n",
        "iou_thres = 0.45  # NMS IOU threshold\n",
        "max_det = 1000  # maximum detections per image\n",
        "classes = None\n",
        "agnostic_nms = False  # class-agnostic NMS\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "# [TODO] draw predictions (see detect.py:L178)\n",
        "from utils.general import non_max_suppression\n",
        "\n",
        "conf_thres = 0.25  # confidence threshold\n",
        "iou_thres = 0.45  # NMS IOU threshold\n",
        "max_det = 1000  # maximum detections per image\n",
        "classes = None\n",
        "agnostic_nms = False  # class-agnostic NMS\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "# [TODO] draw predictions (see detect.py:L178)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def draw_predictions(imgs, pred, class_names):\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
        "    ax.imshow(imgs[0].permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "    for *xyxy, conf, cls in pred[0]:\n",
        "        label = f'{class_names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, ax, label=label, color=(1, 0, 0), line_thickness=2)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_one_box(x, ax, label=None, color=(1, 0, 0), line_thickness=2):\n",
        "    # Plots one bounding box on the image\n",
        "    tl = line_thickness or round(0.002 * (x[2] - x[0] + x[3] - x[1]) / 2) + 1  # line thickness\n",
        "    x = [float(i) for i in x]\n",
        "    xyxy = [x[0], x[1], x[2] - x[0], x[3] - x[1]]\n",
        "    rect = patches.Rectangle(xyxy[:2], *xyxy[2:], linewidth=tl, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    if label:\n",
        "        ax.text(x[0], x[1], label, color=color, fontsize=12, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "# [TODO] Draw predictions\n",
        "# Example usage (assuming predictions are made and `imgs` and `pred` are available):\n",
        "draw_predictions(imgs, pred, model.names)\n",
        "\n",
        "# Forward\n",
        "with torch.cuda.amp.autocast(amp):\n",
        "    pred = model(imgs)  # forward\n",
        "    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
        "    if opt.quad:\n",
        "        loss *= 4.0\n",
        "\n",
        "# [TODO] Draw predictions\n",
        "draw_predictions(imgs, pred, model.names)\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def draw_predictions(img, pred, class_names):\n",
        "    img = img.permute(1, 2, 0).cpu().numpy()  # Convert image tensor to numpy array\n",
        "    img = (img * 255).astype(np.uint8)  # Convert from [0, 1] to [0, 255]\n",
        "    for *xyxy, conf, cls in pred:\n",
        "        label = f'{class_names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, img, label=label, color=(0, 0, 255), line_thickness=2)\n",
        "    return img\n",
        "\n",
        "def plot_one_box(x, img, label=None, color=(1, 0, 0), line_thickness=2):\n",
        "    # Plots one bounding box on the image\n",
        "    tl = line_thickness or round(0.002 * (x[2] - x[0] + x[3] - x[1]) / 2) + 1  # line thickness\n",
        "    x = [int(i) for i in x]\n",
        "    cv2.rectangle(img, (x[0], x[1]), (x[2], x[3]), color, thickness=tl)\n",
        "    if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = x[0] + t_size[0], x[1] - t_size[1] - 3\n",
        "        cv2.rectangle(img, (x[0], x[1]), c2, color, -1)  # filled\n",
        "        cv2.putText(img, label, (x[0], x[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "def save_video_with_predictions(video_path, output_path, model, device):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        img = torch.from_numpy(frame).to(device).float() / 255.0\n",
        "        img = img.permute(2, 0, 1).unsqueeze(0)  # Convert to batch format\n",
        "        pred = model(img)[0]  # Get prediction\n",
        "\n",
        "        # Draw predictions on the frame\n",
        "        frame = draw_predictions(img[0], pred, model.names)\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Example usage\n",
        "video_path = 'input_video.mp4'  # Path to input video\n",
        "output_path = 'output_video.mp4'  # Path to save output video\n",
        "save_video_with_predictions(video_path, output_path, model, device)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
